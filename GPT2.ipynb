{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3Mb4HwjEf63xVMvg980Rv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Z4XkkOxe3T",
        "outputId": "3e2d9059-f1e1-4ec6-c2af-4da772e2be77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN8ZVmXp6d_6",
        "outputId": "7a15bf6c-8cf9-4f57-e510-e0d966d9e4a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/GPT2/input.txt\"\n",
        "\n",
        "# open input.txt and save in text\n",
        "with open (path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "# check type\n",
        "type(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len of the dataset\n",
        "print(\"length of dataset in characters :\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yErB-gtdxj8z",
        "outputId": "2e47ff4c-d3a8-4eee-a094-be4bc46d3db7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters : 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"here are all the unique characters that occur in this text\"\"\"\n",
        "chars = sorted(list(set(text))) # sorted list of unique characters\n",
        "vocab_size = len(chars) # length of unique characters\n",
        "# print chars and vocab size\n",
        "print(\" \".join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5vbQNHb603x",
        "outputId": "a7c59e71-c184-4959-8ff1-3ed957fdc71f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "tokeniser\n",
        "One token is One character in this model.\n",
        "we need a strategy to tokenize i.e encode\n",
        "and decode individual characters into integers\n",
        "and back\n",
        "\"\"\"\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s:[stoi[c] for c in s] # encoder : take string, output list of integers\n",
        "decode = lambda l: \"\".join(itos[i] for i in l) # decoder : take a list of intergers, output a string\n"
      ],
      "metadata": {
        "id": "uzQVCBKQ7fz5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"Karan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUtjvXlb8bXx",
        "outputId": "a0f7a802-4327-4126-afb7-6c22b418a618"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[23, 39, 56, 39, 52]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode([0,1,2,3,4,5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vmZfvrNu8euQ",
        "outputId": "2f61a78b-4f33-4e38-d86c-115e410e8768"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# encode text and wrap it in a tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "# check the shape and data type\n",
        "print(data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di7gywbX7meo",
        "outputId": "b91d791f-7808-4a50-dc7d-85eb4a922b96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first 50 elements in data,\n",
        "# which is an encoded representation of the first 50 characters of text\n",
        "print(data[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "absdkxu576tn",
        "outputId": "3cec8725-7154-4228-d17a-289a110d0f9d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the characters in the novel are now represented by integers. The integers are stretched out and wrapped in a tensor. It is the entire encoding of the book."
      ],
      "metadata": {
        "id": "ITrBerwbuKMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting training and test set\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "nu10XHa_uqV2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block Size or Context Length\n",
        "All the text does not get fed into the transformer at once. Instead random chunks from the text gets fed into the transformer sequentially.\n",
        "The size of this chunk that gets fed into the transformer is referred to as context length or block size."
      ],
      "metadata": {
        "id": "foR0jaulyxYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMTa3eeizAMq",
        "outputId": "4e53513c-58b3-4a21-a9d4-900948b0aaf4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size +1]"
      ],
      "metadata": {
        "id": "mT3Gvn4DzxE0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How attention is working\n",
        "for i in range(block_size):\n",
        "  context = x[:i+1] # tensor upto this position\n",
        "  target = y[i] # should predict this tensor\n",
        "  print(f\"when input is {context} the target : {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ApzWvAV0qW5",
        "outputId": "ab84cbab-5335-4853-fbee-bffe8a5cc188"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target : 47\n",
            "when input is tensor([18, 47]) the target : 56\n",
            "when input is tensor([18, 47, 56]) the target : 57\n",
            "when input is tensor([18, 47, 56, 57]) the target : 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target : 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target : 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target : 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target : 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its done this way to ensure the transformer is used to seeing context from just one word, all the way upto block_size and everything in between.\n",
        "\n",
        "This has advantages during inference because the transformer will be able to handle any length of input context\n",
        "\n",
        "After block size, we will need to start truncating. Because the transformer will never have context beyond this limit"
      ],
      "metadata": {
        "id": "uMuqgeiX0wSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many blocks will process in parallel\n",
        "block_size = 8 # maximum context length for predictions"
      ],
      "metadata": {
        "id": "GTSVlDZ4L0s8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Size\n",
        "For compute efficiency, multiple of the block size tensors are stacked together  to form a higher dimensional tensor, which is used as the input to the transformer.  \n",
        "\n",
        "But they dont communicate with each other or share information. They are all just processed in parallel.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-tL0V6Zy1hrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function get_batch\n",
        "  **Arguement** : dataset  \n",
        "  **Return** :   \n",
        "      1. x :  input tensor ,stack of 4 tensors,each tensor has 8 ints  \n",
        "      2. y :  target tensor ,stack of 4 tensors,each tensor has 8 ints\n",
        "\n",
        "\n",
        "    get_batch(split)  \n",
        "      data = train or val, if split = train or val\n",
        "\n",
        "      ix = a tensor of shape (batch_size,) = (4,0) filled with random integers\n",
        "\n",
        "      x = a stack of 4 tensors\n",
        "      Each tensor is 8 characters starting from ix which are random points in the text\n",
        "\n",
        "      y = a stack of 4 tensors\n",
        "      Each tensor is 8 characters starting from ix+1 which are the targets for x in the text\n",
        "\n",
        "      return x and y"
      ],
      "metadata": {
        "id": "MBYn7AzzZHc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a mini batch. which is a batchsize stack of blocks into a singe tensor\n",
        "torch.manual_seed(1337)\n",
        "def get_batch(split):\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "  ix = torch.randint(len(data) - block_size,(batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "8b59dmmh1H9R"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb,yb = get_batch(\"train\")\n",
        "print(f\"inputs : {xb.shape} ,\\n {xb}\")\n",
        "print(f\"targets : {yb.shape} ,\\n {yb}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjJWJvFxNCYE",
        "outputId": "1e71ede5-a50a-413c-8bf6-28032487c60e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs : torch.Size([4, 8]) ,\n",
            " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets : torch.Size([4, 8]) ,\n",
            " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # for our mini-batch\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()}, target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oNi3eqAcZ2b",
        "outputId": "8429d7e3-ff3e-4ca0-baca-1073e2f397bb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is [24], target is 43\n",
            "when input is [24, 43], target is 58\n",
            "when input is [24, 43, 58], target is 5\n",
            "when input is [24, 43, 58, 5], target is 57\n",
            "when input is [24, 43, 58, 5, 57], target is 1\n",
            "when input is [24, 43, 58, 5, 57, 1], target is 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46], target is 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43], target is 39\n",
            "when input is [44], target is 53\n",
            "when input is [44, 53], target is 56\n",
            "when input is [44, 53, 56], target is 1\n",
            "when input is [44, 53, 56, 1], target is 58\n",
            "when input is [44, 53, 56, 1, 58], target is 46\n",
            "when input is [44, 53, 56, 1, 58, 46], target is 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39], target is 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58], target is 1\n",
            "when input is [52], target is 58\n",
            "when input is [52, 58], target is 1\n",
            "when input is [52, 58, 1], target is 58\n",
            "when input is [52, 58, 1, 58], target is 46\n",
            "when input is [52, 58, 1, 58, 46], target is 39\n",
            "when input is [52, 58, 1, 58, 46, 39], target is 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58], target is 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1], target is 46\n",
            "when input is [25], target is 17\n",
            "when input is [25, 17], target is 27\n",
            "when input is [25, 17, 27], target is 10\n",
            "when input is [25, 17, 27, 10], target is 0\n",
            "when input is [25, 17, 27, 10, 0], target is 21\n",
            "when input is [25, 17, 27, 10, 0, 21], target is 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1], target is 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54], target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-Gram Language Models\n",
        "The N-gram language model uses bayes theorem to calculate the probability of a word occuring, given the previous word occured.\n",
        "\n",
        "Bi-Gram : When last 1 word is used for prediction\n",
        "Tri-Gram : When last 2 words are used for prediction  \n",
        "n-gram : when last n words are used\n"
      ],
      "metadata": {
        "id": "cZ3LMBV54z-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,inputs,targets):\n",
        "\n",
        "    #inputs and targets are both (B,T) tensors of ints\n",
        "    logits = self.token_embedding_table(inputs) # (Batch,Time,Channels) (4,8,65)\n",
        "    B , T , C = logits.shape\n",
        "    # reshaping to the form required by crossentropy function\n",
        "    logits = logits.view(B*T, C)\n",
        "    targets = targets.view(B*T)\n",
        "    loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits,loss = m(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQsQMUk45Ajw",
        "outputId": "471e2a32-438e-496a-e512-69e267919280"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the bigram class\n",
        "\n",
        "initialize an object token embedding table of size (vocab_size,vocab_size) and fills them with random weights.\n",
        "\n",
        "map the input values to the corresponding row of the table_size, and arrange them in another tensor. stack the 4 tensors together\n",
        "logits = tensor of dim (batch_size,block_size,vocab_size)\n"
      ],
      "metadata": {
        "id": "rNwKYmaJ_nJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function\n",
        "\n",
        "A function to understand the quality of predictions.\n",
        "\n",
        "negative-log-liklihood loss\n",
        "\n",
        "implements under name cross-entropy in pytorch"
      ],
      "metadata": {
        "id": "cUPGQB9EBkbX"
      }
    }
  ]
}